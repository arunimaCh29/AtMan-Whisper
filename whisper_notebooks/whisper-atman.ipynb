{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 130\u001b[0m\n\u001b[1;32m    122\u001b[0m waveform \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(audio_sample)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[1;32m    124\u001b[0m transform \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mMelSpectrogram(\n\u001b[1;32m    125\u001b[0m     sample_rate\u001b[38;5;241m=\u001b[39msample_rate,\n\u001b[1;32m    126\u001b[0m     n_mels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m,\n\u001b[1;32m    127\u001b[0m     hop_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m160\u001b[39m,\n\u001b[1;32m    128\u001b[0m     n_fft\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m\n\u001b[1;32m    129\u001b[0m )\n\u001b[0;32m--> 130\u001b[0m input_audio \u001b[38;5;241m=\u001b[39m transform(waveform)\u001b[38;5;241m.\u001b[39mlog2()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch and channel dimensions\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Load the Whisper model and create an instance of WhisperWithATMAN\u001b[39;00m\n\u001b[1;32m    133\u001b[0m whisper_model \u001b[38;5;241m=\u001b[39m WhisperForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/whisper-tiny\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.12/site-packages/torchaudio/transforms/_transforms.py:620\u001b[0m, in \u001b[0;36mMelSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m    waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;124;03m    Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    619\u001b[0m specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspectrogram(waveform)\n\u001b[0;32m--> 620\u001b[0m mel_specgram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmel_scale(specgram)\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mel_specgram\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.12/site-packages/torchaudio/transforms/_transforms.py:412\u001b[0m, in \u001b[0;36mMelScale.forward\u001b[0;34m(self, specgram)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;124;03m    specgram (Tensor): A spectrogram STFT of dimension (..., freq, time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;124;03m    Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# (..., time, freq) dot (freq, n_mels) -> (..., n_mels, time)\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m mel_specgram \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(specgram\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfb)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mel_specgram\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "class WhisperWithATMAN(nn.Module):\n",
    "    def __init__(self, original_whisper_model):\n",
    "        super().__init__()\n",
    "        self.whisper = original_whisper_model\n",
    "\n",
    "    def forward(self, x, token_index=None, suppression_factor=0.1, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Forward method modified to include ATMAN-based perturbation for explainability.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor representing the Log-Mel spectrograms.\n",
    "            token_index (int, optional): Index of the token to perturb.\n",
    "            suppression_factor (float, optional): Factor by which attention scores are suppressed.\n",
    "            threshold (float, optional): Cosine similarity threshold for correlated token suppression.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Decoded output after perturbation.\n",
    "        \"\"\"\n",
    "        # Step 1: Encode the input audio\n",
    "        encoder_outputs = self.whisper.model.encoder(x)\n",
    "        embeddings = encoder_outputs.last_hidden_state\n",
    "\n",
    "        # Step 2: Calculate attention scores\n",
    "        attention_scores = self.compute_attention_scores(embeddings)\n",
    "        \n",
    "        # Step 3: Apply attention modification if a token index is provided\n",
    "        if token_index is not None:\n",
    "            similarity_matrix = self.calculate_cosine_similarity(embeddings)\n",
    "            modified_attention_scores = self.modify_attention_scores(\n",
    "                attention_scores, token_index, suppression_factor, similarity_matrix, threshold\n",
    "            )\n",
    "        else:\n",
    "            modified_attention_scores = attention_scores\n",
    "\n",
    "        # Step 4: Decode with modified attention\n",
    "        decoder_outputs = self.whisper.model.decoder(\n",
    "            inputs_embeds=embeddings, \n",
    "            encoder_hidden_states=encoder_outputs.last_hidden_state,\n",
    "            encoder_attention_mask=modified_attention_scores\n",
    "        )\n",
    "\n",
    "        return decoder_outputs\n",
    "\n",
    "    def compute_attention_scores(self, embeddings):\n",
    "        \"\"\"\n",
    "        Computes attention scores from embeddings using scaled dot-product attention.\n",
    "\n",
    "        Args:\n",
    "            embeddings (torch.Tensor): Embeddings from the encoder.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Attention score matrix.\n",
    "        \"\"\"\n",
    "        Q = self.whisper.model.encoder.layer[0].attention.self.query(embeddings)\n",
    "        K = self.whisper.model.encoder.layer[0].attention.self.key(embeddings)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (embeddings.size(-1) ** 0.5)\n",
    "        return F.softmax(scores, dim=-1)\n",
    "\n",
    "    def modify_attention_scores(self, H, token_index, suppression_factor, similarity_matrix=None, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Modifies attention scores to suppress specific tokens and their correlated tokens.\n",
    "\n",
    "        Args:\n",
    "            H (torch.Tensor): Original attention score matrix.\n",
    "            token_index (int): Index of the token to suppress.\n",
    "            suppression_factor (float): Factor by which to suppress attention scores.\n",
    "            similarity_matrix (torch.Tensor, optional): Cosine similarity matrix for embeddings.\n",
    "            threshold (float, optional): Threshold for identifying correlated tokens.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Modified attention score matrix.\n",
    "        \"\"\"\n",
    "        s, _ = H.shape[-2:]  # Sequence length\n",
    "        mask = torch.ones((s, s), device=H.device)\n",
    "        mask[token_index, :] *= (1 - suppression_factor)\n",
    "\n",
    "        if similarity_matrix is not None:\n",
    "            correlated_mask = (similarity_matrix > threshold).float() * (1 - suppression_factor)\n",
    "            mask += correlated_mask\n",
    "        \n",
    "        return H * mask\n",
    "\n",
    "    def calculate_cosine_similarity(self, embeddings):\n",
    "        \"\"\"\n",
    "        Calculates the cosine similarity matrix for the given embeddings.\n",
    "\n",
    "        Args:\n",
    "            embeddings (torch.Tensor): Encoder embeddings.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Cosine similarity matrix.\n",
    "        \"\"\"\n",
    "        normalized_embeddings = F.normalize(embeddings, p=2, dim=-1)\n",
    "        similarity_matrix = torch.matmul(normalized_embeddings, normalized_embeddings.transpose(-2, -1))\n",
    "        return similarity_matrix\n",
    "\n",
    "# Function to load and preprocess audio\n",
    "def preprocess_audio(file_path):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    transform = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mels=80,\n",
    "        hop_length=160,\n",
    "        n_fft=400\n",
    "    )\n",
    "    log_mel_spec = transform(waveform).log2()\n",
    "    return log_mel_spec.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Load dataset from Hugging Face and preprocess\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "# Preprocess the first audio sample from the dataset\n",
    "audio_sample = dataset[0][\"audio\"][\"array\"]\n",
    "sample_rate = dataset[0][\"audio\"][\"sampling_rate\"]\n",
    "waveform = torch.tensor(audio_sample).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "transform = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_mels=80,\n",
    "    hop_length=160,\n",
    "    n_fft=400\n",
    ")\n",
    "input_audio = transform(waveform).log2().unsqueeze(0)  # Add batch and channel dimensions\n",
    "\n",
    "# Load the Whisper model and create an instance of WhisperWithATMAN\n",
    "whisper_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "atman_whisper = WhisperWithATMAN(whisper_model)\n",
    "\n",
    "# Run the preprocessed audio sample through the model\n",
    "output = atman_whisper(input_audio, token_index=5, suppression_factor=0.3)\n",
    "\n",
    "# Print the output tokens\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
